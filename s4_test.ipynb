{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qhduan/Work/cpu/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from s3_train import build_model\n",
    "from utils import MAX_LENGTH, LABEL_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_ld = {v: k for k, v in LABEL_DICT.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = pickle.load(open('ws.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (MAX_LENGTH,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qhduan/Work/cpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/qhduan/Work/cpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/qhduan/Work/cpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     3092100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 200, 4)       1204        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 200, 4)       2404        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 200, 4)       3604        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 200, 4)       4804        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 200, 4)       6004        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 200, 4)       7204        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 200, 1)       5           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 200, 1)       5           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 200, 1)       5           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 200, 1)       5           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 200, 1)       5           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 200, 1)       5           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200, 6)       0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1200)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1200)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1200)         1441200     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 1200)         0           activation_1[0][0]               \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          120100      multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100)          400         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 100)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          10100       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100)          400         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 100)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,690,261\n",
      "Trainable params: 4,689,861\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model, atten_model = build_model(shape, len(ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sentence):\n",
    "    vec = ws.transform(sentence, MAX_LENGTH)\n",
    "#     print(vec)\n",
    "    x = np.array([vec])\n",
    "    pred = model.predict(x)\n",
    "    atten = atten_model.predict(x)\n",
    "    atten = atten.reshape((MAX_LENGTH, 6))\n",
    "    atten = np.sum(atten, axis=-1)\n",
    "    print(atten.shape)\n",
    "    \n",
    "    ret = []\n",
    "    for i in range(len(LABEL_DICT)):\n",
    "        ret.append((\n",
    "            inverse_ld[i],\n",
    "            pred[0][i] * 100\n",
    "        ))\n",
    "    ret = sorted(ret, key=lambda x: x[1], reverse=True)\n",
    "    for r in ret:\n",
    "        print('{}\\t\\t{:.2f}%'.format(\n",
    "            r[0],\n",
    "            r[1]\n",
    "        ))\n",
    "#     for i, w in enumerate(list(sentence)):\n",
    "#         print(atten[i], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_test(data, max_length=MAX_LENGTH):\n",
    "    while True:\n",
    "        ind = np.random.randint(0, len(data))\n",
    "        x = data[ind][0]\n",
    "        y = data[ind][2]\n",
    "        if len(x) <= max_length:\n",
    "            break\n",
    "    print(x)\n",
    "    print(y)\n",
    "    test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💛好想找人聊天阿我。粒粒，你姐我看到你开心我就开心，珍惜你的现在。丁丁，找一下上次我们一起看中的那副眼睛的地址，我们三个一人一副，然后暑假戴着去旅游，型爆啦。哈，其实肉麻的说一句，认识到你们两个，是我这辈子的幸运，我最好的闺蜜。此生真的死而无憾！💛\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t97.24%\n",
      "neutral\t\t2.02%\n",
      "sad\t\t0.38%\n",
      "angry\t\t0.18%\n",
      "fear\t\t0.11%\n",
      "disgust\t\t0.08%\n",
      "suprise\t\t0.00%\n",
      "今天去西安博物馆，看到一对武警巡逻。一个哥们大喊一声，向右转，那群武警真的向右转了，包括那位队长。过了三秒，队长觉的不对，一回头，那位哥们跑了，跑的真快。。。。\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t31.88%\n",
      "neutral\t\t22.28%\n",
      "sad\t\t14.57%\n",
      "angry\t\t13.09%\n",
      "disgust\t\t12.39%\n",
      "fear\t\t5.40%\n",
      "suprise\t\t0.40%\n",
      "等我发了大财、出了大名、一不小心混进上流社会、告别下流人生，我一定给中国的成功人士做个表率：保持朴素纯真，以平常心，走平常路，行平常事，做平常人，最多偷偷带着老婆孩子情人相好隐姓埋名偷偷溜到国外享受几下高尚生活，绝不张扬！这个...我真能做到吗？\n",
      "happy\n",
      "(200,)\n",
      "suprise\t\t54.88%\n",
      "disgust\t\t24.08%\n",
      "happy\t\t6.47%\n",
      "fear\t\t5.10%\n",
      "angry\t\t4.61%\n",
      "neutral\t\t3.78%\n",
      "sad\t\t1.08%\n",
      "15岁觉得游泳难，放弃游泳，到18岁遇到一个你喜欢的人约你去游泳，你只好说“我不会”。18岁觉得英文难，放弃英文，28岁出现一个很棒但要会英文的工作，你只好说“我不会”。人生前期越嫌麻烦，越懒得学，后来就越可能错过让你动心的人和事，错过新风景。——蔡康永\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t99.57%\n",
      "neutral\t\t0.25%\n",
      "sad\t\t0.06%\n",
      "angry\t\t0.05%\n",
      "disgust\t\t0.04%\n",
      "fear\t\t0.03%\n",
      "suprise\t\t0.00%\n",
      "【遇到不会读的字，怎么用拼音打？】可以先打个“u”然后打各个部首的读音，就能出来哦！我相信你之前不知道的……via-广东特搜 -大话东莞 【东莞猛料，关注-】~~~\n",
      "happy\n",
      "(200,)\n",
      "neutral\t\t58.73%\n",
      "happy\t\t27.59%\n",
      "angry\t\t4.99%\n",
      "sad\t\t4.08%\n",
      "fear\t\t2.39%\n",
      "disgust\t\t1.71%\n",
      "suprise\t\t0.52%\n",
      "我们来看看济南的活动，手机体验区也很火爆啊！看吧，亲子团都上阵了，瞧这父子俩，再看看 小弟弟专注的样子，玩得真是不亦乐乎啊！济南咱们联想乐Phone的活动在万达沃尔玛购物广场喔\n",
      "happy\n",
      "(200,)\n",
      "angry\t\t49.50%\n",
      "happy\t\t20.39%\n",
      "neutral\t\t11.55%\n",
      "sad\t\t10.55%\n",
      "disgust\t\t6.88%\n",
      "fear\t\t0.94%\n",
      "suprise\t\t0.19%\n",
      "我大使馆被轰炸，你抗议；美国向台出售武器，你又抗议；印度参与南海油田开采，你仍然抗议；日本拘留我船长，你再三抗议；菲律宾要强占我黄岩岛，你他妈还是抗议。当什邡人民对你扔了个矿泉水瓶子，你终于拿起了武器。\n",
      "sad\n",
      "(200,)\n",
      "sad\t\t82.96%\n",
      "angry\t\t7.46%\n",
      "fear\t\t3.66%\n",
      "happy\t\t3.23%\n",
      "neutral\t\t2.37%\n",
      "disgust\t\t0.30%\n",
      "suprise\t\t0.01%\n",
      "跟亲爱的游完泳在吃宵夜，然后接到爸爸电话问我在干嘛，我说刚去游泳了，他发出很惊奇的声音＂呀嗬~这么厉害啊？那有无游五十米啊？＂我说拜托，花了老娘十五块大洋就游50m那还不如不去！老头子还继续说＂呀~看来以后要对我就女儿刮目相看啦！居然能游50m了！＂是有多瞧不起我呀！\n",
      "disgust\n",
      "(200,)\n",
      "disgust\t\t93.51%\n",
      "happy\t\t3.37%\n",
      "angry\t\t1.80%\n",
      "neutral\t\t0.45%\n",
      "suprise\t\t0.43%\n",
      "sad\t\t0.30%\n",
      "fear\t\t0.13%\n",
      "【Chanel推出呼啦圈造型手袋】老佛爷透露，他的设计初衷是为时尚女性在度假时提供一个好用的背包。他在接受《每日电讯》网站采访时表示：“这是一款为海边度假打造的产品。你要一个能塞下浴巾的包吧？你可以把它随手放在沙滩上，还能在包上挂东西。”这款包容量大，材质轻，而通常两者很难得兼\n",
      "happy\n",
      "(200,)\n",
      "neutral\t\t53.66%\n",
      "happy\t\t17.58%\n",
      "sad\t\t9.41%\n",
      "angry\t\t8.51%\n",
      "fear\t\t7.73%\n",
      "disgust\t\t2.32%\n",
      "suprise\t\t0.80%\n",
      "蓝色西装，情侣搭，英伦风。真心爱这一对的搭配！，用帽子挡脸，好萌好有爱~搭配的时候，可别忘了捎上你爱的TA哦~\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t53.47%\n",
      "neutral\t\t43.62%\n",
      "sad\t\t1.15%\n",
      "fear\t\t0.78%\n",
      "angry\t\t0.71%\n",
      "disgust\t\t0.23%\n",
      "suprise\t\t0.05%\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
