{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qhduan/Work/cpu/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from s3_train import build_model\n",
    "from utils import MAX_LENGTH, LABEL_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_ld = {v: k for k, v in LABEL_DICT.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = pickle.load(open('ws.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (MAX_LENGTH,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/qhduan/Work/cpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/qhduan/Work/cpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/qhduan/Work/cpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     3092100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 200, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200, 64)      63936       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 12800)        0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 200)          2560200     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 64, 200)      0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 200, 64)      0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 200, 64)      0           bidirectional_1[0][0]            \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 12800)        0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          1280100     flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100)          400         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          10100       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100)          400         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 100)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7)            707         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,007,943\n",
      "Trainable params: 7,007,543\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model, atten_model = build_model(shape, len(ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model_gru_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sentence):\n",
    "    vec = ws.transform(sentence, MAX_LENGTH)\n",
    "#     print(vec)\n",
    "    x = np.array([vec])\n",
    "    pred = model.predict(x)\n",
    "    atten = atten_model.predict(x)\n",
    "    atten = atten.reshape((MAX_LENGTH, 64))\n",
    "    atten = np.sum(atten, axis=-1)\n",
    "    print(atten.shape)\n",
    "    \n",
    "    ret = []\n",
    "    for i in range(len(LABEL_DICT)):\n",
    "        ret.append((\n",
    "            inverse_ld[i],\n",
    "            pred[0][i] * 100\n",
    "        ))\n",
    "    ret = sorted(ret, key=lambda x: x[1], reverse=True)\n",
    "    for r in ret:\n",
    "        print('{}\\t\\t{:.2f}%'.format(\n",
    "            r[0],\n",
    "            r[1]\n",
    "        ))\n",
    "#     for i, w in enumerate(list(sentence)):\n",
    "#         print(atten[i], w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_test(data, max_length=MAX_LENGTH):\n",
    "    while True:\n",
    "        ind = np.random.randint(0, len(data))\n",
    "        x = data[ind][0]\n",
    "        y = data[ind][2]\n",
    "        if len(x) <= max_length:\n",
    "            break\n",
    "    print(x)\n",
    "    print(y)\n",
    "    test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“元芳，最近中游的2012电子竞技大会又要开启了，此事你怎么看？”“大人，这背后恐怕有天大的奖励啊！”“此话怎讲？”“往年这活动，参与人数就多，人一多，奖励太少就拿不出手了。”“你的意思是老夫应该去看看？斗地主、升级打台球老夫不在行，但是象棋... ，老夫还是有自信的！”“大人英明”\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t72.94%\n",
      "sad\t\t16.92%\n",
      "fear\t\t5.00%\n",
      "neutral\t\t2.68%\n",
      "angry\t\t2.31%\n",
      "disgust\t\t0.13%\n",
      "suprise\t\t0.02%\n",
      "【连续七天下雨，一直下到五一放假！】五一想去哪儿玩？找个阳光灿烂的地方发呆！那请远离广州！今晚起，广州天天下雨，连下七天！！阵雨、中雨、大雨、暴雨。。。怨念啊。。。\n",
      "angry\n",
      "(200,)\n",
      "angry\t\t99.85%\n",
      "happy\t\t0.13%\n",
      "sad\t\t0.01%\n",
      "neutral\t\t0.01%\n",
      "disgust\t\t0.00%\n",
      "suprise\t\t0.00%\n",
      "fear\t\t0.00%\n",
      "【彩虹休闲一刻】病人：“大夫，你给我说实话，我是不是活不了几天了？” 医生：“谁说的？你不要胡思乱想。” 病人：“你别骗我，我都知道了，昨天主治医生来查房的时侯，我在看报纸，他莫名其妙的对我说：哟，还在看连载的啊！”\n",
      "disgust\n",
      "(200,)\n",
      "disgust\t\t98.39%\n",
      "happy\t\t1.40%\n",
      "sad\t\t0.08%\n",
      "angry\t\t0.06%\n",
      "neutral\t\t0.05%\n",
      "suprise\t\t0.02%\n",
      "fear\t\t0.01%\n",
      "究竟有多少人重視這投票呢???某人都飆到29了!!我就不相信依人數我們會輸!!各位有空的親幫忙投吧!!羅祥現在是71名!!衝衝衝!!!!!!不管它真實性!但真的是面子問題阿!!!\n",
      "angry\n",
      "(200,)\n",
      "angry\t\t98.58%\n",
      "happy\t\t1.11%\n",
      "neutral\t\t0.15%\n",
      "sad\t\t0.10%\n",
      "disgust\t\t0.04%\n",
      "suprise\t\t0.01%\n",
      "fear\t\t0.00%\n",
      "真正快乐的人，不外两种：一种是真正了解宇宙奥秘和人生百态，把一切都看透了的人；另一种是乐天轻松，，不知烦恼为何物的人。而我们中的大多数，多游荡在两者之间。各位，！ \n",
      "happy\n",
      "(200,)\n",
      "neutral\t\t55.21%\n",
      "happy\t\t44.29%\n",
      "angry\t\t0.26%\n",
      "sad\t\t0.12%\n",
      "disgust\t\t0.08%\n",
      "suprise\t\t0.02%\n",
      "fear\t\t0.02%\n",
      "有哪位好心的程序猿GG帮这位MM翻译一下么？ 好吧，当有一些没有受过教育的人骂你，不做任何事，因为生活是不公平的，你要去适应它，你要知道，那些人不可能成为一个男人之中的王子，所以…\n",
      "happy\n",
      "(200,)\n",
      "neutral\t\t73.59%\n",
      "happy\t\t24.08%\n",
      "suprise\t\t1.16%\n",
      "fear\t\t0.50%\n",
      "angry\t\t0.50%\n",
      "disgust\t\t0.09%\n",
      "sad\t\t0.08%\n",
      "我们也许不小心在自己的喜欢的工作上成了所谓的明星，其实和很多人一样，只是普通人家的孩子，而且本身也会有很多的不足，一路面对问题，改正，努力，不断进步，所以我们才会越来越好。大家都好好的，放平心态，面对自己每天的挑战和美好、真实的生活，你会越来越快乐\n",
      "happy\n",
      "(200,)\n",
      "neutral\t\t67.37%\n",
      "happy\t\t32.40%\n",
      "sad\t\t0.13%\n",
      "angry\t\t0.06%\n",
      "fear\t\t0.02%\n",
      "disgust\t\t0.01%\n",
      "suprise\t\t0.01%\n",
      "长春有只“混社会”的小狗，它叫多多。如果它选定你了，就会站起来先用小爪子挠挠你的小腿，“扑通”一声坐到你面前，挺起上身、两只前爪合在一起，开始作揖。如果你把钱给它，它便一口叼住，撒开腿直奔卖店，买它最喜欢吃的零食。\n",
      "suprise\n",
      "(200,)\n",
      "suprise\t\t100.00%\n",
      "happy\t\t0.00%\n",
      "neutral\t\t0.00%\n",
      "angry\t\t0.00%\n",
      "fear\t\t0.00%\n",
      "disgust\t\t0.00%\n",
      "sad\t\t0.00%\n",
      "还有不到24小时欧洲杯就要开赛了，准确来说应该是16个小时，波兰与希腊的对决，预计波兰2比0希腊1，对于波兰库巴右边的攻击力还是很有信心的！至于另一场，预计俄罗斯1比0胜捷克！\n",
      "neutral\n",
      "(200,)\n",
      "neutral\t\t74.50%\n",
      "happy\t\t25.35%\n",
      "fear\t\t0.08%\n",
      "sad\t\t0.03%\n",
      "angry\t\t0.02%\n",
      "suprise\t\t0.02%\n",
      "disgust\t\t0.00%\n",
      "我们准备了很多好玩的，答谢粉丝支持的活动例如这次！我们专门为粉丝开通了微信账号，重点是里头会有“神秘小礼品”的获取方法！玩微博太闷啦？可以去微信找我们chat chat喔~~~（快用手机微信扫描下面二维码加我们为好友啦~）\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t74.85%\n",
      "angry\t\t15.48%\n",
      "neutral\t\t7.61%\n",
      "disgust\t\t1.40%\n",
      "sad\t\t0.45%\n",
      "suprise\t\t0.13%\n",
      "fear\t\t0.08%\n",
      "，喜欢就好，说到这个我和店主遇到一个挺无语的人，压根没有用过，就质疑东西是假的。哎……再次说明一下，请不要问我东西是否正品，这个问题我再三强调，我团的保证正品的情况下最低价或者很低的价格，如果是假的，我会负责到底。如果假你们可以联系我，或者跟店主沟通。还有说一下要互相尊重。\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t57.38%\n",
      "neutral\t\t16.55%\n",
      "disgust\t\t13.05%\n",
      "angry\t\t7.03%\n",
      "sad\t\t5.49%\n",
      "fear\t\t0.33%\n",
      "suprise\t\t0.16%\n",
      "【食用沙門氏菌污染芒果 加州73人生病】据美国食品安全新闻网报道，近日美国爆发了由芒果引起的沙门氏菌疫情，已造成加州73人感染。加州卫生局发言人表示，有67%的感染者称曾食用过芒果，然而目前当局尚未摸清问题芒果的品牌以及出产地。最近大家还是不要吃芒果了吧！\n",
      "fear\n",
      "(200,)\n",
      "fear\t\t53.32%\n",
      "suprise\t\t40.56%\n",
      "disgust\t\t3.15%\n",
      "angry\t\t1.66%\n",
      "sad\t\t0.56%\n",
      "happy\t\t0.45%\n",
      "neutral\t\t0.31%\n",
      "不經意看到了評論 這麽曲折的都被我看到了 是註定的吧 嗯 傻……你傻？嗯 是吧 是，你傻，我活生生把你騙了……多可憐的小男孩啊 就這樣被我騙了三年啊……真傻那又是誰 謊騙我 讓我相信他 ？你傻 還是我傻？呵呵 最近睡得夠晚的了，太痛了……嗯 晚安\n",
      "fear\n",
      "(200,)\n",
      "fear\t\t100.00%\n",
      "happy\t\t0.00%\n",
      "neutral\t\t0.00%\n",
      "sad\t\t0.00%\n",
      "suprise\t\t0.00%\n",
      "angry\t\t0.00%\n",
      "disgust\t\t0.00%\n",
      "【英艺术家编纂“谷歌词典” 半数配图与色情暴力有关】据英《每日邮报》5月31日报道，英国金斯顿大学两名艺术家菲利克斯·海斯（Felix Heyes）和本·韦斯特（Ben West）日前编纂了一本收录了21000个单词的“谷歌词典”，每个词都用谷歌搜索的第一张图释义，最后发现词典内半数图片都与色情暴力有关。\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t99.93%\n",
      "neutral\t\t0.07%\n",
      "angry\t\t0.00%\n",
      "fear\t\t0.00%\n",
      "suprise\t\t0.00%\n",
      "sad\t\t0.00%\n",
      "disgust\t\t0.00%\n",
      "我勒个去，我很意外的被关注了……莫非看出我有可点化的地方？这下我的绳命可能会梗加井猜了！大师，弟子最想请教的事是：明天凌晨英格兰和意大利的欧洲杯比赛，谁赢？欧洲杯最后谁夺冠？只能选一个队，您可别选梭油的梭油~\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t56.95%\n",
      "neutral\t\t35.34%\n",
      "angry\t\t3.10%\n",
      "disgust\t\t2.81%\n",
      "sad\t\t1.48%\n",
      "fear\t\t0.18%\n",
      "suprise\t\t0.14%\n",
      "不错我喜欢！volvo里个人觉得外观品相不错的小车。黑白配的比较得体。heico的拉花不错，小logo时总觉得怪怪的，放大才明白，真的很像尿不湿。小车挺工整，和你交友不差。给2分，美滋滋去吧\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t49.46%\n",
      "neutral\t\t47.85%\n",
      "disgust\t\t1.61%\n",
      "angry\t\t0.61%\n",
      "sad\t\t0.28%\n",
      "suprise\t\t0.10%\n",
      "fear\t\t0.09%\n",
      "tina：到访过数间糖百府分店，份量最大的是佐敦店，最少（味道最淡）的是深水埗店，最宽厂的是九龙城店，人流最多的是铜锣湾店，最浓味的是旺角黑布街店。必尝招牌菜：宇治金时、芝麻糊拼杏仁糊、烧仙草冰、麻甩\n",
      "happy\n",
      "(200,)\n",
      "neutral\t\t62.18%\n",
      "happy\t\t37.53%\n",
      "sad\t\t0.12%\n",
      "angry\t\t0.11%\n",
      "disgust\t\t0.03%\n",
      "fear\t\t0.02%\n",
      "suprise\t\t0.02%\n",
      "【曝光】via@苏堤的西湖：海博出租又拒载了。从南丹路斯波顿出来，拉开停在路边的出租车门，司机问我去那，我说去遵义路。他说不去，只好下车！挑路线，上次是下雨加价。海博真牛！\n",
      "angry\n",
      "(200,)\n",
      "angry\t\t87.08%\n",
      "sad\t\t8.38%\n",
      "happy\t\t3.29%\n",
      "disgust\t\t0.74%\n",
      "neutral\t\t0.43%\n",
      "fear\t\t0.06%\n",
      "suprise\t\t0.01%\n",
      "《赛尔号：异星精灵传说》周末放送，限时48小时的免费惊喜即将结束了（截止今日18:00）。还没下载的小赛尔们抓紧时间啦，时间一点点的逝去，惊喜也渐渐远离。机不可失，失不再来哦！\n",
      "happy\n",
      "(200,)\n",
      "happy\t\t51.06%\n",
      "neutral\t\t47.85%\n",
      "angry\t\t0.44%\n",
      "sad\t\t0.30%\n",
      "disgust\t\t0.24%\n",
      "fear\t\t0.07%\n",
      "suprise\t\t0.05%\n",
      "我们总是有做不完的工作，吐不完的怨气，减不完的肥，草不完的大爷，骂不完的SB，坑不完的爹，这就是生活。。。喜欢XXOO，你懂得，请关注\n",
      "neutral\n",
      "(200,)\n",
      "neutral\t\t65.86%\n",
      "happy\t\t30.55%\n",
      "sad\t\t1.79%\n",
      "disgust\t\t0.70%\n",
      "angry\t\t0.55%\n",
      "fear\t\t0.50%\n",
      "suprise\t\t0.05%\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    random_test(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
